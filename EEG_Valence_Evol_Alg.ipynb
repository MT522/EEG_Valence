{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1b3f54-7d1b-48b6-b1a7-03e674e39bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io as sio\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b7e4a-3a7c-461c-9c60-bdbca81090ae",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9364b5ab-2504-46ed-8b54-6bb0207ee4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('Project_data.mat')\n",
    "channels = data['Channels']\n",
    "test_data = data['TestData']\n",
    "train_data = data['TrainData']\n",
    "train_labels = data['TrainLabels'].ravel()\n",
    "fs = data['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f4be3c-dbe2-4e35-9da6-0d40647349e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550,), (59, 5000, 550), (59, 5000, 159))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41696288-a525-4d4d-8ba4-e0fda0731e9b",
   "metadata": {},
   "source": [
    "### Load Pre-Calculated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c7fc18-7d75-4168-ace9-634873515895",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.load('var.npy')\n",
    "amp_hist = np.load('amp_hist.npy')\n",
    "ar_model = np.load('ar_model.npy')\n",
    "cross_corr = np.load('cross_corr.npy')\n",
    "max_freq = np.load('max_freq.npy')\n",
    "mean_freq = np.load('mean_freq.npy')\n",
    "med_freq = np.load('med_freq.npy')\n",
    "rel_energy = np.load('rel_energy.npy')\n",
    "\n",
    "var_test = np.load('var_test.npy')\n",
    "amp_hist_test = np.load('amp_hist_test.npy')\n",
    "ar_model_test = np.load('ar_model_test.npy')\n",
    "cross_corr_test = np.load('cross_corr_test.npy')\n",
    "max_freq_test = np.load('max_freq_test.npy')\n",
    "mean_freq_test = np.load('mean_freq_test.npy')\n",
    "med_freq_test = np.load('med_freq_test.npy')\n",
    "rel_energy_test = np.load('rel_energy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bedfb14-337c-43a2-9675-023e4e8b3aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 5369), (159, 5369))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features_tr = np.concatenate((var, amp_hist, ar_model, cross_corr, max_freq, mean_freq, med_freq, rel_energy), axis = 1)\n",
    "features_te = np.concatenate((var_test, amp_hist_test, ar_model_test, cross_corr_test, max_freq_test, mean_freq_test, med_freq_test, rel_energy_test), axis = 1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "features_tr = sc.fit_transform(features_tr)\n",
    "features_te = sc.fit_transform(features_te)\n",
    "\n",
    "np.save('features_train_scaled', features_tr)\n",
    "np.save('features_test_scaled', features_te)\n",
    "\n",
    "features_tr.shape, features_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d83245c-aebd-4096-b26a-4281c543338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5369, 'ar_192')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_fea = ['variance_' + str(i) for i in range(var.shape[1])]\n",
    "hist_fea = ['hist_' + str(i) for i in range(amp_hist.shape[1])]\n",
    "ar_fea = ['ar_' + str(i) for i in range(ar_model.shape[1])]\n",
    "cross_fea = ['correlation_' + str(i) for i in range(cross_corr.shape[1])]\n",
    "max_fea = ['max_freq_' + str(i) for i in range(max_freq.shape[1])]\n",
    "mean_fea = ['mean_freq_' + str(i) for i in range(mean_freq.shape[1])]\n",
    "med_fea = ['median_freq_' + str(i) for i in range(med_freq.shape[1])]\n",
    "energ_fea = ['rel_energy_' + str(i) for i in range(rel_energy.shape[1])]\n",
    "\n",
    "all_fea = var_fea + hist_fea + ar_fea + cross_fea + max_fea + mean_fea + med_fea + energ_fea\n",
    "\n",
    "def get_feature_name(index):\n",
    "    return all_fea[index]\n",
    "\n",
    "len(all_fea), get_feature_name(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35a8c02-c5c6-43fc-858b-fbd82ea551af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 5369), (550,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197aaa24-3ca1-4fda-a950-05aee2700e9c",
   "metadata": {},
   "source": [
    "### PSO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29ad9a63-b977-4b8c-947d-49158b97a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Particle:\n",
    "    def __init__(self, num_features, total_features):\n",
    "        self.position = np.random.choice(total_features, size=(num_features,), replace=False)\n",
    "        self.velocity = np.random.rand(num_features)\n",
    "        self.pbest_position = self.position\n",
    "        self.pbest_value = -float('inf')\n",
    "\n",
    "    def update_velocity(self, gbest_position, w=0.5, c1=1, c2=2):\n",
    "        r1 = np.random.rand(len(self.velocity))\n",
    "        r2 = np.random.rand(len(self.velocity))\n",
    "        \n",
    "        cognitive_velocity = c1 * r1 * (self.pbest_position - self.position)\n",
    "        social_velocity = c2 * r2 * (gbest_position - self.position)\n",
    "        \n",
    "        self.velocity = w * self.velocity + cognitive_velocity + social_velocity\n",
    "\n",
    "    def update_position(self, total_features):\n",
    "        old_position = self.position.copy()\n",
    "        self.position = self.position + self.velocity\n",
    "        \n",
    "        clipped_indices = np.where((self.position < 0) | (self.position > total_features-1))\n",
    "        self.position = np.clip(self.position, 0, total_features-1)  # Ensure the position is within the bounds\n",
    "        self.position = np.round(self.position).astype(int)  # Ensure the position is an integer\n",
    "    \n",
    "        # Negate the velocity for clipped positions\n",
    "        self.velocity[clipped_indices] = -self.velocity[clipped_indices]\n",
    "\n",
    "\n",
    "class Swarm:\n",
    "    def __init__(self, num_particles, num_features, total_features):\n",
    "        self.particles = [Particle(num_features, total_features) for _ in range(num_particles)]\n",
    "        self.gbest_value = -float('inf')\n",
    "        self.gbest_position = np.random.choice(total_features, size=(num_features,), replace=False)\n",
    "\n",
    "    def update_gbest(self, X, y):\n",
    "        for particle in self.particles:\n",
    "            fitness_cadidate = self.fitness(particle.position, X, y)\n",
    "            if(particle.pbest_value < fitness_cadidate):\n",
    "                particle.pbest_value = fitness_cadidate\n",
    "                particle.pbest_position = particle.position\n",
    "\n",
    "            if(self.gbest_value < fitness_cadidate):\n",
    "                self.gbest_value = fitness_cadidate\n",
    "                self.gbest_position = particle.position\n",
    "\n",
    "    def move_particles(self, total_features):\n",
    "        for particle in self.particles:\n",
    "            particle.update_velocity(self.gbest_position)\n",
    "            particle.update_position(total_features)\n",
    "\n",
    "    def fitness(self, position, X, y):\n",
    "        X = X[:, position]\n",
    "        mu1 = np.mean(X[y == -1, :], axis=0)\n",
    "        mu2 = np.mean(X[y == 1, :], axis=0)\n",
    "        mu0 = np.mean(X, axis=0)\n",
    "    \n",
    "        S1 = np.sum([(x_i - mu1).reshape(-1, 1) @ (x_i - mu1).reshape(1, -1) for x_i in X[y == -1, :]], axis=0)\n",
    "        S2 = np.sum([(x_i - mu2).reshape(-1, 1) @ (x_i - mu2).reshape(1, -1) for x_i in X[y == 1, :]], axis=0)\n",
    "    \n",
    "        Sb = np.sum([(mu_i - mu0).reshape(-1, 1) @ (mu_i - mu0).reshape(1, -1) for mu_i in [mu1, mu2]], axis=0)\n",
    "    \n",
    "        Sw = S1 + S2\n",
    "    \n",
    "        J = np.trace(Sb) / np.trace(Sw)\n",
    "    \n",
    "        return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9b261ef-a8c3-49ff-ac46-aa3670c67af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 50\n",
    "total_features = 5369\n",
    "num_particles = 1000\n",
    "num_iterations = 100\n",
    "\n",
    "swarm = Swarm(num_particles, num_features, total_features)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    swarm.move_particles(total_features)\n",
    "    swarm.update_gbest(features_tr, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ae00a7c-483f-4ab7-8654-637d3424cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('features_PSO', swarm.gbest_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92fd6ed-a5e4-4c5b-a3dc-8ece75146fc1",
   "metadata": {},
   "source": [
    "#### Select the given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "228e1c13-448f-4c91-9c01-948d60eb6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = features_tr[:, swarm.gbest_position]\n",
    "X_te = features_te[:, swarm.gbest_position]\n",
    "y_tr = train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a89e7-83e6-48f9-975e-526339da88d2",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f95d320-917a-4e5f-bd0d-0876ab3eefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden1_size) \n",
    "        self.l2 = nn.Linear(hidden1_size, hidden2_size)  \n",
    "        self.l3 = nn.Linear(hidden2_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.l1(x))\n",
    "        out = self.relu(self.l2(out))\n",
    "        out = self.sigmoid(self.l3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41e846-051e-480f-b2b7-8ff52b87e45a",
   "metadata": {},
   "source": [
    "#### Hyper Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d97d46a-f7b4-4d21-b546-614b5293b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "input_size = 50\n",
    "hidden1_size = 60\n",
    "hidden2_size = 60\n",
    "num_epochs = 10\n",
    "num_total_steps = 550 // batch_size\n",
    "\n",
    "model = MLPNet(input_size, hidden1_size, hidden2_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babd5da-537f-4f23-a83e-3244c9d47c74",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaf54108-868c-4eed-a4df-750a5df07635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.7001\n",
      "Epoch [2/10], Step [1/55], Loss: 0.4845\n",
      "Epoch [3/10], Step [1/55], Loss: 0.5434\n",
      "Epoch [4/10], Step [1/55], Loss: 0.3442\n",
      "Epoch [5/10], Step [1/55], Loss: 0.3051\n",
      "Epoch [6/10], Step [1/55], Loss: 0.6284\n",
      "Epoch [7/10], Step [1/55], Loss: 0.6700\n",
      "Epoch [8/10], Step [1/55], Loss: 0.4119\n",
      "Epoch [9/10], Step [1/55], Loss: 0.3153\n",
      "Epoch [10/10], Step [1/55], Loss: 0.5139\n",
      "Train Accuracy: 86.59%\n",
      "--------------------------------\n",
      "Validation Accuracy: 78.18%\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.4689\n",
      "Epoch [2/10], Step [1/55], Loss: 0.4388\n",
      "Epoch [3/10], Step [1/55], Loss: 0.0599\n",
      "Epoch [4/10], Step [1/55], Loss: 0.2328\n",
      "Epoch [5/10], Step [1/55], Loss: 0.0398\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1157\n",
      "Epoch [7/10], Step [1/55], Loss: 0.1947\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1810\n",
      "Epoch [9/10], Step [1/55], Loss: 0.2232\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1229\n",
      "Train Accuracy: 96.36%\n",
      "--------------------------------\n",
      "Validation Accuracy: 81.82%\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.2839\n",
      "Epoch [2/10], Step [1/55], Loss: 0.2189\n",
      "Epoch [3/10], Step [1/55], Loss: 0.2584\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1540\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1565\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0194\n",
      "Epoch [7/10], Step [1/55], Loss: 0.0466\n",
      "Epoch [8/10], Step [1/55], Loss: 0.0262\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1547\n",
      "Epoch [10/10], Step [1/55], Loss: 0.0302\n",
      "Train Accuracy: 92.50%\n",
      "--------------------------------\n",
      "Validation Accuracy: 83.64%\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.2538\n",
      "Epoch [2/10], Step [1/55], Loss: 0.0382\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1293\n",
      "Epoch [4/10], Step [1/55], Loss: 0.2897\n",
      "Epoch [5/10], Step [1/55], Loss: 0.0162\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0111\n",
      "Epoch [7/10], Step [1/55], Loss: 0.0730\n",
      "Epoch [8/10], Step [1/55], Loss: 0.0426\n",
      "Epoch [9/10], Step [1/55], Loss: 0.0036\n",
      "Epoch [10/10], Step [1/55], Loss: 0.0034\n",
      "Train Accuracy: 99.55%\n",
      "--------------------------------\n",
      "Validation Accuracy: 94.55%\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.0022\n",
      "Epoch [2/10], Step [1/55], Loss: 0.0053\n",
      "Epoch [3/10], Step [1/55], Loss: 0.0465\n",
      "Epoch [4/10], Step [1/55], Loss: 0.2762\n",
      "Epoch [5/10], Step [1/55], Loss: 0.2472\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0106\n",
      "Epoch [7/10], Step [1/55], Loss: 0.0090\n",
      "Epoch [8/10], Step [1/55], Loss: 0.0043\n",
      "Epoch [9/10], Step [1/55], Loss: 0.0627\n",
      "Epoch [10/10], Step [1/55], Loss: 0.0196\n",
      "Train Accuracy: 98.18%\n",
      "--------------------------------\n",
      "Validation Accuracy: 97.27%\n",
      "--------------------------------\n",
      "\n",
      "Overal Results\n",
      "Average Training Accurcy is : 94.63636363636364\n",
      "Average Validation Accurcy is : 87.0909090909091\n"
     ]
    }
   ],
   "source": [
    "X_tr_tensor = torch.tensor(np.float32(X_tr))\n",
    "y_tr_tensor = torch.tensor(np.float32(np.where(y_tr == 1, 1.0, 0.0)))\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X_tr_tensor)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=valid_subsampler)\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "            for i, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs.ravel(), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if (i+1) % num_total_steps-1 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{num_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "    print(f'Train Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracies.append(100 * correct / total)\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('\\nOveral Results')\n",
    "print(f'Average Training Accurcy is : {np.mean(train_accuracies)}')\n",
    "print(f'Average Validation Accurcy is : {np.mean(valid_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ace3dbe-c7ef-470a-8b96-cecb2c4fd4a6",
   "metadata": {},
   "source": [
    "#### Predict Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68e5c328-e3a7-462b-afa7-4c6987dcf493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "         0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor = torch.tensor(np.float32(X_te)).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data_tensor)\n",
    "        \n",
    "predicted = torch.where(outputs > 0.5, 1, 0).T\n",
    "np.save('test_labels_MLP_PSO', predicted.cpu())\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a204fc-8ae3-47e2-bd81-b1208a60d953",
   "metadata": {},
   "source": [
    "### RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "74298112-b773-4b6b-85dd-ae6df59b282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, X_train):\n",
    "        super().__init__()\n",
    "        kmeans = KMeans(n_clusters=output_dim)\n",
    "        kmeans.fit(X_train)\n",
    "        self.centers = nn.Parameter(torch.tensor(kmeans.cluster_centers_).float())\n",
    "        self.beta = nn.Parameter(torch.ones(1, output_dim))\n",
    "\n",
    "    def radial_function(self, x):\n",
    "        x = x.unsqueeze(-1)  # add an extra dimension at the end for broadcasting\n",
    "        return torch.exp(-self.beta.mul((x - self.centers.T).pow(2).sum(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.radial_function(x)\n",
    "\n",
    "class RBFNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, X_train):\n",
    "        super().__init__()\n",
    "        self.rbf = RBF(input_dim, hidden_dim, X_train)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rbf(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e9e51-9e1c-4d59-b8b7-6816543af582",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "196d914a-ddc6-4774-905e-c695cee673e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "input_dim = 50 \n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "num_epochs = 10\n",
    "num_total_steps = 550 // batch_size\n",
    "\n",
    "model = RBFNet(input_dim, hidden_dim, output_dim, torch.tensor(np.float32(X_tr))).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfbf9a7-6edd-4e9c-a4c6-811802c2e78b",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a9f91e4c-45de-40de-b918-72f666f22968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1570\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1117\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1628\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1902\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1396\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1234\n",
      "Epoch [7/10], Step [1/55], Loss: 0.1807\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1752\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1572\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1462\n",
      "Train Accuracy: 70.91%\n",
      "--------------------------------\n",
      "Validation Accuracy: 71.82%\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1577\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1655\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1982\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1837\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1499\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1825\n",
      "Epoch [7/10], Step [1/55], Loss: 0.2034\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1929\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1219\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1700\n",
      "Train Accuracy: 75.00%\n",
      "--------------------------------\n",
      "Validation Accuracy: 63.64%\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1602\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1412\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1192\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1461\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1266\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1086\n",
      "Epoch [7/10], Step [1/55], Loss: 0.2413\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1704\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1625\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1520\n",
      "Train Accuracy: 74.32%\n",
      "--------------------------------\n",
      "Validation Accuracy: 67.27%\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1526\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1120\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1469\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1726\n",
      "Epoch [5/10], Step [1/55], Loss: 0.0987\n",
      "Epoch [6/10], Step [1/55], Loss: 0.2027\n",
      "Epoch [7/10], Step [1/55], Loss: 0.1311\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1858\n",
      "Epoch [9/10], Step [1/55], Loss: 0.2294\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1835\n",
      "Train Accuracy: 76.82%\n",
      "--------------------------------\n",
      "Validation Accuracy: 75.45%\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1178\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1274\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1165\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1432\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1406\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1584\n",
      "Epoch [7/10], Step [1/55], Loss: 0.2016\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1756\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1828\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1782\n",
      "Train Accuracy: 72.73%\n",
      "--------------------------------\n",
      "Validation Accuracy: 67.27%\n",
      "--------------------------------\n",
      "\n",
      "Overal Results\n",
      "Average Training Accurcy is : 73.95454545454545\n",
      "Average Validation Accurcy is : 69.09090909090908\n"
     ]
    }
   ],
   "source": [
    "X_tr_tensor = torch.tensor(np.float32(X_tr))\n",
    "y_tr_tensor = torch.tensor(np.float32(np.where(y_tr == 1, 1.0, 0.0)))\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X_tr_tensor)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=valid_subsampler)\n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "            for i, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs.ravel(), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if (i+1) % num_total_steps-1 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{num_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "    print(f'Train Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracies.append(100 * correct / total)\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('\\nOveral Results')\n",
    "print(f'Average Training Accurcy is : {np.mean(train_accuracies)}')\n",
    "print(f'Average Validation Accurcy is : {np.mean(valid_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70d131-5b57-4b38-9c8d-b44a396b6807",
   "metadata": {},
   "source": [
    "#### Predict Test Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6887845f-d371-46b9-a0f5-3e60b6bfe1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "         0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "         0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor = torch.tensor(np.float32(X_te)).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data_tensor)\n",
    "        \n",
    "predicted = torch.where(outputs > 0.5, 1, 0).T\n",
    "np.save('test_labels_RBF_PSO', predicted.cpu())\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
