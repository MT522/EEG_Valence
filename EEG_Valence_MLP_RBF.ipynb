{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8d4c5c-2492-4894-8651-4acbc13e59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import io as sio\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0226992b-10b2-4493-9c1e-9566975e66be",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cad2081-5676-4024-aaad-85d3fafd346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('Project_data.mat')\n",
    "channels = data['Channels']\n",
    "test_data = data['TestData']\n",
    "train_data = data['TrainData']\n",
    "train_labels = data['TrainLabels'].ravel()\n",
    "fs = data['fs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b64d437-f650-411e-9de2-69901768687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550,), (59, 5000, 550), (59, 5000, 159))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape, train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749db5ba-a951-46e1-89a5-3b01ac2ed6f1",
   "metadata": {},
   "source": [
    "### Functions to Extract Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "398b0413-4811-4924-a317-29f575d300f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import simps\n",
    "\n",
    "def calc_variance(data):\n",
    "    return np.std(data, axis=1).T\n",
    "\n",
    "def calc_amp_hist(data, num_bins):\n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "    features = np.zeros((num_trials, num_channels*(num_bins+1)))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):\n",
    "            features[i, j*(num_bins+1):(j+1)*(num_bins+1)] = np.histogram(data[j, :, i], num_bins)[1]\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_ar(data, model_size, fit_intercept):\n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "    signal_length = data.shape[1]\n",
    "\n",
    "    features = np.zeros((num_trials, num_channels*(model_size + (lambda x: 1 if x else 0)(fit_intercept))))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):\n",
    "            signal = data[j, :, i]\n",
    "\n",
    "            feature_mat = np.zeros((signal_length-model_size, model_size))\n",
    "            for k in range(model_size, signal_length):\n",
    "                feature_mat[k-model_size, :] = signal[k-model_size:k]\n",
    "\n",
    "            lr = LinearRegression(fit_intercept=fit_intercept)\n",
    "            lr.fit(feature_mat, signal[model_size:])\n",
    "\n",
    "            if fit_intercept:\n",
    "                features[i, j*(model_size+1):(j+1)*(model_size+1)] = np.insert(lr.coef_, 0, lr.intercept_)\n",
    "            else:\n",
    "                features[i, j*(model_size):(j+1)*(model_size)] = lr.coef_\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_correlation(data):\n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "\n",
    "    features = np.zeros((num_trials, num_channels**2))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):\n",
    "            for k in range(num_channels):\n",
    "                mean1 = np.mean(data[j, :, i])\n",
    "                mean2 = np.mean(data[k, :, i])\n",
    "\n",
    "                features[i, j*num_channels + k] = np.mean((data[j, :, i] - mean1)*(data[k, :, i])-mean2)\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_max_freq(data, fs): \n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "\n",
    "    features = np.zeros((num_trials, num_channels))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):      \n",
    "            frequencies, psd = welch(data[j, :, i], fs=fs, nperseg=2048)\n",
    "            frequencies = frequencies.ravel()\n",
    "            features[i, j] = frequencies[np.argmax(psd)]\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_mean_freq(data, fs): \n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "\n",
    "    features = np.zeros((num_trials, num_channels))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):      \n",
    "            frequencies, psd = welch(data[j, :, i], fs=fs, nperseg=2048)\n",
    "            frequencies = frequencies.ravel()\n",
    "            features[i, j] = np.sum(frequencies * psd) / np.sum(psd)\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_median_freq(data, fs): \n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "\n",
    "    features = np.zeros((num_trials, num_channels))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):      \n",
    "            frequencies, psd = welch(data[j, :, i], fs=fs, nperseg=2048)\n",
    "            frequencies = frequencies.ravel()\n",
    "            cumulative_psd = np.cumsum(psd)\n",
    "            median_index = np.where(cumulative_psd >= cumulative_psd[-1] / 2)[0][0]\n",
    "\n",
    "            features[i, j] = frequencies[median_index]\n",
    "\n",
    "    return features\n",
    "\n",
    "def calc_rel_energy(data, fs, bands):\n",
    "    num_channels = data.shape[0]\n",
    "    num_trials = data.shape[2]\n",
    "    \n",
    "    features = np.zeros((num_trials, num_channels*len(bands.keys())))\n",
    "    for i in range(num_trials):\n",
    "        for j in range(num_channels):\n",
    "            frequencies, psd = welch(data[j, :, i], fs=fs, nperseg=2048)\n",
    "            frequencies = frequencies.ravel()\n",
    "\n",
    "            total_energy = simps(psd, frequencies)\n",
    "            \n",
    "            for k, (band, (low, high)) in enumerate(bands.items()):\n",
    "                idx_band = np.logical_and(frequencies >= low, frequencies <= high)\n",
    "                band_energy = simps(psd[idx_band], frequencies[idx_band], axis=0)\n",
    "                relative_band_energy = band_energy / total_energy\n",
    "\n",
    "                features[i, j*len(bands.keys()) + k] = relative_band_energy\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb189a90-c803-4e30-a47a-e24a11043f30",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae6283d0-b38b-4c6d-afff-7dd1286bf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_no_intercept = calc_ar(train_data, 10, False) #Takes lots of time to run!! Load the pre-calculated data instead!\n",
    "np.save('ar_no_intercept', ar_no_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0e2e3bf-2763-406c-9fd3-6b548a33a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr = calc_correlation(train_data) #Takes lots of time to run!! Load the pre-calculated data instead!\n",
    "np.save('cross_corr', cross_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ce40a-9008-468b-8c7e-0c096e5a5413",
   "metadata": {},
   "source": [
    "#### Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c851921-bf92-4795-9945-273b812a19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = calc_variance(train_data)\n",
    "amp_hist = calc_amp_hist(train_data, 10)\n",
    "ar_model = np.load('ar_no_intercept.npy')\n",
    "cross_corr = np.load('cross_corr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa0873f-8e9f-47c4-a6f1-5172b43bdbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_test = calc_variance(test_data)\n",
    "amp_hist_test = calc_amp_hist(test_data, 10)\n",
    "ar_model_test = calc_ar(test_data, 10, False)\n",
    "cross_corr_test = calc_correlation(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "120d18a5-ee7b-476e-9c1e-8a59231eb4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 59), (550, 649), (550, 590), (550, 3481))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.shape, amp_hist.shape, ar_model.shape, cross_corr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28552cb-ed12-4967-9e39-047e98d23fe5",
   "metadata": {},
   "source": [
    "#### Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1eda93-a45d-45a6-92c7-7887be0d86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = {\n",
    "    'Delta': (0.1, 4),\n",
    "    'Theta': (4, 8),\n",
    "    'Alpha': (8, 12),\n",
    "    'Low-Range Beta': (12, 16),\n",
    "    'Mid-Range Beta': (16, 21),\n",
    "    'High-Range Beta': (21, 30),\n",
    "    'Gamma': (30, 500)\n",
    "}\n",
    "\n",
    "max_freq = calc_max_freq(train_data, fs)\n",
    "mean_freq = calc_mean_freq(train_data, fs)\n",
    "med_freq = calc_median_freq(train_data, fs)\n",
    "rel_energy = calc_rel_energy(train_data, fs, bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75238637-55bf-4582-bf49-f1eb70d06487",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_freq_test = calc_max_freq(test_data, fs)\n",
    "mean_freq_test = calc_mean_freq(test_data, fs)\n",
    "med_freq_test = calc_median_freq(test_data, fs)\n",
    "rel_energy_test = calc_rel_energy(test_data, fs, bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dcb6b43-82bb-4d90-a29e-e7a8ea2a69eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 59), (550, 59), (550, 59), (550, 413))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq.shape, mean_freq.shape, med_freq.shape, rel_energy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5f957-a382-4a5c-8979-37639de55c8e",
   "metadata": {},
   "source": [
    "#### Aggregate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fe0a1c-0461-4fe4-8f43-2f4d0416c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('var', var)\n",
    "np.save('amp_hist', amp_hist)\n",
    "np.save('ar_model', ar_model)\n",
    "np.save('cross_corr', cross_corr)\n",
    "np.save('max_freq', max_freq)\n",
    "np.save('mean_freq', mean_freq)\n",
    "np.save('med_freq', med_freq)\n",
    "np.save('rel_energy', rel_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c0d8f60-efb9-4733-ba82-774f645e8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('var_test', var_test)\n",
    "np.save('amp_hist_test', amp_hist_test)\n",
    "np.save('ar_model_test', ar_model_test)\n",
    "np.save('cross_corr_test', cross_corr_test)\n",
    "np.save('max_freq_test', max_freq_test)\n",
    "np.save('mean_freq_test', mean_freq_test)\n",
    "np.save('med_freq_test', med_freq_test)\n",
    "np.save('rel_energy_test', rel_energy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e7cfbe-0635-44e3-9745-b8069a941c05",
   "metadata": {},
   "source": [
    "##### JUST LOAD EVERY THING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88ed45f-563a-4d06-8d5c-3d382f02c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = np.load('var.npy')\n",
    "amp_hist = np.load('amp_hist.npy')\n",
    "ar_model = np.load('ar_model.npy')\n",
    "cross_corr = np.load('cross_corr.npy')\n",
    "max_freq = np.load('max_freq.npy')\n",
    "mean_freq = np.load('mean_freq.npy')\n",
    "med_freq = np.load('med_freq.npy')\n",
    "rel_energy = np.load('rel_energy.npy')\n",
    "\n",
    "var_test = np.load('var_test.npy')\n",
    "amp_hist_test = np.load('amp_hist_test.npy')\n",
    "ar_model_test = np.load('ar_model_test.npy')\n",
    "cross_corr_test = np.load('cross_corr_test.npy')\n",
    "max_freq_test = np.load('max_freq_test.npy')\n",
    "mean_freq_test = np.load('mean_freq_test.npy')\n",
    "med_freq_test = np.load('med_freq_test.npy')\n",
    "rel_energy_test = np.load('rel_energy_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a1acf4-c93b-4ef2-9500-4015c1749712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 5369), (159, 5369))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features_tr = np.concatenate((var, amp_hist, ar_model, cross_corr, max_freq, mean_freq, med_freq, rel_energy), axis = 1)\n",
    "features_te = np.concatenate((var_test, amp_hist_test, ar_model_test, cross_corr_test, max_freq_test, mean_freq_test, med_freq_test, rel_energy_test), axis = 1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "features_tr = sc.fit_transform(features_tr)\n",
    "features_te = sc.fit_transform(features_te)\n",
    "\n",
    "np.save('features_train_scaled', features_tr)\n",
    "np.save('features_test_scaled', features_te)\n",
    "\n",
    "features_tr.shape, features_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc3289b-4981-4234-8f9d-2efa930f17d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5369, 'ar_192')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_fea = ['variance_' + str(i) for i in range(var.shape[1])]\n",
    "hist_fea = ['hist_' + str(i) for i in range(amp_hist.shape[1])]\n",
    "ar_fea = ['ar_' + str(i) for i in range(ar_model.shape[1])]\n",
    "cross_fea = ['correlation_' + str(i) for i in range(cross_corr.shape[1])]\n",
    "max_fea = ['max_freq_' + str(i) for i in range(max_freq.shape[1])]\n",
    "mean_fea = ['mean_freq_' + str(i) for i in range(mean_freq.shape[1])]\n",
    "med_fea = ['median_freq_' + str(i) for i in range(med_freq.shape[1])]\n",
    "energ_fea = ['rel_energy_' + str(i) for i in range(rel_energy.shape[1])]\n",
    "\n",
    "all_fea = var_fea + hist_fea + ar_fea + cross_fea + max_fea + mean_fea + med_fea + energ_fea\n",
    "\n",
    "def get_feature_name(index):\n",
    "    return all_fea[index]\n",
    "\n",
    "len(all_fea), get_feature_name(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d780823-de11-4cae-b496-66e49c0c0664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 5369), (550,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tr.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70166a-2bbd-4b9e-8a14-65db2c92f26e",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acdcc02-bf02-4ecf-8134-a6e8e384c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature indices: [ 152  273  329  892  931  932 1002 1021 1022 1031 1101 1121 1152 1262\n",
      " 1448 1521 1684 1818 2054 2241 2502 2734 3124 3188 3192 3300 3772 4015\n",
      " 4138 4140 4142 4576 4584 4632 4735 4818 4825 4832 4860 4919 4943 4950\n",
      " 4952 5111 5116 5234 5300 5314 5349 5363]\n",
      "Selected features: ['hist_93', 'hist_214', 'hist_270', 'ar_184', 'ar_223', 'ar_224', 'ar_294', 'ar_313', 'ar_314', 'ar_323', 'ar_393', 'ar_413', 'ar_444', 'ar_554', 'correlation_150', 'correlation_223', 'correlation_386', 'correlation_520', 'correlation_756', 'correlation_943', 'correlation_1204', 'correlation_1436', 'correlation_1826', 'correlation_1890', 'correlation_1894', 'correlation_2002', 'correlation_2474', 'correlation_2717', 'correlation_2840', 'correlation_2842', 'correlation_2844', 'correlation_3278', 'correlation_3286', 'correlation_3334', 'correlation_3437', 'max_freq_39', 'max_freq_46', 'max_freq_53', 'mean_freq_22', 'median_freq_22', 'median_freq_46', 'median_freq_53', 'median_freq_55', 'rel_energy_155', 'rel_energy_160', 'rel_energy_278', 'rel_energy_344', 'rel_energy_358', 'rel_energy_393', 'rel_energy_407']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "def fisher_score(X, y):\n",
    "    mean = np.mean(X, axis=0)\n",
    "\n",
    "    sb1 = (np.sum(X[y==1], axis=0) - mean)**2\n",
    "    sb2 = (np.sum(X[y==-1], axis=0) - mean)**2\n",
    "\n",
    "    sw1 = np.var(X[y==1], axis=0)\n",
    "    sw2 = np.var(X[y==-1], axis=0)\n",
    "    \n",
    "    return sb1 + sb2 / sw1 + sw2\n",
    "\n",
    "selector = SelectKBest(score_func=fisher_score, k=50)\n",
    "X_tr = selector.fit_transform(features_tr, train_labels)\n",
    "\n",
    "# Get the indices of the selected features\n",
    "selected_indices = np.where(selector.get_support())[0]\n",
    "print(f'Selected feature indices: {selected_indices}')\n",
    "print(f'Selected features: {[get_feature_name(index) for index in selected_indices]}')\n",
    "\n",
    "X_te = features_te[:, selected_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d3a423-3565-4f3e-8c61-b15f6bbde9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((550, 50), (159, 50))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, X_te.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0fee43-3f56-48a0-bfbf-623a10fcf6a4",
   "metadata": {},
   "source": [
    "### Prepare the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc94c84a-4cf5-4bbb-bff0-f965959a8703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden1_size) \n",
    "        self.l2 = nn.Linear(hidden1_size, hidden2_size)  \n",
    "        self.l3 = nn.Linear(hidden2_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.l1(x))\n",
    "        out = self.relu(self.l2(out))\n",
    "        out = self.sigmoid(self.l3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8916ed5-529e-429e-abfd-81d5f6c20da3",
   "metadata": {},
   "source": [
    "#### Hyper Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab0ac643-91e5-4762-a029-cb53582cce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "input_size = 50\n",
    "hidden1_size = 10\n",
    "hidden2_size = 10\n",
    "num_epochs = 10\n",
    "num_total_steps = 550 // batch_size\n",
    "\n",
    "model = MLPNet(input_size, hidden1_size, hidden2_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adbff8-38d3-476e-a918-b48770c956e5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fcda784-b130-4a37-9bac-6d9480059f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.7627\n",
      "Epoch [2/10], Step [1/55], Loss: 0.5265\n",
      "Epoch [3/10], Step [1/55], Loss: 0.5833\n",
      "Epoch [4/10], Step [1/55], Loss: 0.4459\n",
      "Epoch [5/10], Step [1/55], Loss: 0.3059\n",
      "Epoch [6/10], Step [1/55], Loss: 0.6078\n",
      "Epoch [7/10], Step [1/55], Loss: 0.3925\n",
      "Epoch [8/10], Step [1/55], Loss: 0.3953\n",
      "Epoch [9/10], Step [1/55], Loss: 0.2519\n",
      "Epoch [10/10], Step [1/55], Loss: 0.3415\n",
      "Train Accuracy: 86.14%\n",
      "--------------------------------\n",
      "Validation Accuracy: 70.91%\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.4066\n",
      "Epoch [2/10], Step [1/55], Loss: 0.2848\n",
      "Epoch [3/10], Step [1/55], Loss: 0.1387\n",
      "Epoch [4/10], Step [1/55], Loss: 0.2430\n",
      "Epoch [5/10], Step [1/55], Loss: 0.1774\n",
      "Epoch [6/10], Step [1/55], Loss: 0.1967\n",
      "Epoch [7/10], Step [1/55], Loss: 0.1757\n",
      "Epoch [8/10], Step [1/55], Loss: 0.2686\n",
      "Epoch [9/10], Step [1/55], Loss: 0.2818\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1097\n",
      "Train Accuracy: 90.45%\n",
      "--------------------------------\n",
      "Validation Accuracy: 80.00%\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.4358\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1985\n",
      "Epoch [3/10], Step [1/55], Loss: 0.2806\n",
      "Epoch [4/10], Step [1/55], Loss: 0.1767\n",
      "Epoch [5/10], Step [1/55], Loss: 0.2299\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0321\n",
      "Epoch [7/10], Step [1/55], Loss: 0.2540\n",
      "Epoch [8/10], Step [1/55], Loss: 0.0977\n",
      "Epoch [9/10], Step [1/55], Loss: 0.1424\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1309\n",
      "Train Accuracy: 97.05%\n",
      "--------------------------------\n",
      "Validation Accuracy: 85.45%\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.0494\n",
      "Epoch [2/10], Step [1/55], Loss: 0.1033\n",
      "Epoch [3/10], Step [1/55], Loss: 0.0557\n",
      "Epoch [4/10], Step [1/55], Loss: 0.0749\n",
      "Epoch [5/10], Step [1/55], Loss: 0.0207\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0193\n",
      "Epoch [7/10], Step [1/55], Loss: 0.0592\n",
      "Epoch [8/10], Step [1/55], Loss: 0.1974\n",
      "Epoch [9/10], Step [1/55], Loss: 0.0929\n",
      "Epoch [10/10], Step [1/55], Loss: 0.1420\n",
      "Train Accuracy: 97.50%\n",
      "--------------------------------\n",
      "Validation Accuracy: 87.27%\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/10], Step [1/55], Loss: 0.1171\n",
      "Epoch [2/10], Step [1/55], Loss: 0.0390\n",
      "Epoch [3/10], Step [1/55], Loss: 0.0754\n",
      "Epoch [4/10], Step [1/55], Loss: 0.0873\n",
      "Epoch [5/10], Step [1/55], Loss: 0.0493\n",
      "Epoch [6/10], Step [1/55], Loss: 0.0407\n",
      "Epoch [7/10], Step [1/55], Loss: 0.0362\n",
      "Epoch [8/10], Step [1/55], Loss: 0.0164\n",
      "Epoch [9/10], Step [1/55], Loss: 0.0882\n",
      "Epoch [10/10], Step [1/55], Loss: 0.0113\n",
      "Train Accuracy: 96.59%\n",
      "--------------------------------\n",
      "Validation Accuracy: 92.73%\n",
      "--------------------------------\n",
      "\n",
      "Overal Results\n",
      "Average Training Accurcy is : 93.54545454545455\n",
      "Average Validation Accurcy is : 83.27272727272728\n"
     ]
    }
   ],
   "source": [
    "X_tr_tensor = torch.tensor(np.float32(X_tr))\n",
    "y_tr_tensor = torch.tensor(np.float32(np.where(train_labels == 1, 1.0, 0.0)))\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X_tr_tensor)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=valid_subsampler)\n",
    "\n",
    "    for epoch in range(num_epochs): \n",
    "            for i, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs.ravel(), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if (i+1) % num_total_steps-1 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{num_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "    print(f'Train Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracies.append(100 * correct / total)\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('\\nOveral Results')\n",
    "print(f'Average Training Accurcy is : {np.mean(train_accuracies)}')\n",
    "print(f'Average Validation Accurcy is : {np.mean(valid_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927e70a-7913-434d-a096-5dfbbfa4587f",
   "metadata": {},
   "source": [
    "#### Predict Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0ccd61-2d1b-4483-aa17-8890a75a34ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor = torch.tensor(np.float32(X_te)).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data_tensor)\n",
    "        \n",
    "predicted = torch.where(outputs > 0.5, 1, 0).T\n",
    "np.save('test_labels_MLP', predicted.cpu())\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cf60d5-caa8-4479-ae45-e950bd800ee4",
   "metadata": {},
   "source": [
    "### Prepare RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dd0fa24-868c-4e97-b357-34085b5757d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class RBF(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, X_train):\n",
    "        super().__init__()\n",
    "        kmeans = KMeans(n_clusters=output_dim)\n",
    "        kmeans.fit(X_train)\n",
    "        self.centers = nn.Parameter(torch.tensor(kmeans.cluster_centers_).float())\n",
    "        self.beta = nn.Parameter(torch.ones(1, output_dim))\n",
    "\n",
    "    def radial_function(self, x):\n",
    "        x = x.unsqueeze(-1)  # add an extra dimension at the end for broadcasting\n",
    "        return torch.exp(-self.beta.mul((x - self.centers.T).pow(2).sum(1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.radial_function(x)\n",
    "\n",
    "class RBFNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, X_train):\n",
    "        super().__init__()\n",
    "        self.rbf = RBF(input_dim, hidden_dim, X_train)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rbf(x)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e354b60-4cb0-4ee5-91f7-71f326876756",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "144b7257-4529-4e89-96d6-e03424b61330",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "input_dim = 50 \n",
    "hidden_dim = 2\n",
    "output_dim = 1\n",
    "num_epochs = 15\n",
    "num_total_steps = 550 // batch_size\n",
    "\n",
    "model = RBFNet(input_dim, hidden_dim, output_dim, torch.tensor(np.float32(X_tr))).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b009b133-587c-4dbe-9a5f-77860c5c2a07",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e6346a7-0cbe-4208-b9f7-a8b910caf7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch [1/15], Step [1/55], Loss: 0.2322\n",
      "Epoch [2/15], Step [1/55], Loss: 0.2491\n",
      "Epoch [3/15], Step [1/55], Loss: 0.2432\n",
      "Epoch [4/15], Step [1/55], Loss: 0.2584\n",
      "Epoch [5/15], Step [1/55], Loss: 0.2615\n",
      "Epoch [6/15], Step [1/55], Loss: 0.2480\n",
      "Epoch [7/15], Step [1/55], Loss: 0.2376\n",
      "Epoch [8/15], Step [1/55], Loss: 0.2290\n",
      "Epoch [9/15], Step [1/55], Loss: 0.2280\n",
      "Epoch [10/15], Step [1/55], Loss: 0.2441\n",
      "Epoch [11/15], Step [1/55], Loss: 0.2375\n",
      "Epoch [12/15], Step [1/55], Loss: 0.2381\n",
      "Epoch [13/15], Step [1/55], Loss: 0.2649\n",
      "Epoch [14/15], Step [1/55], Loss: 0.2413\n",
      "Epoch [15/15], Step [1/55], Loss: 0.2489\n",
      "Train Accuracy: 52.50%\n",
      "--------------------------------\n",
      "Validation Accuracy: 54.55%\n",
      "--------------------------------\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch [1/15], Step [1/55], Loss: 0.2405\n",
      "Epoch [2/15], Step [1/55], Loss: 0.2128\n",
      "Epoch [3/15], Step [1/55], Loss: 0.2286\n",
      "Epoch [4/15], Step [1/55], Loss: 0.2404\n",
      "Epoch [5/15], Step [1/55], Loss: 0.1996\n",
      "Epoch [6/15], Step [1/55], Loss: 0.2525\n",
      "Epoch [7/15], Step [1/55], Loss: 0.2548\n",
      "Epoch [8/15], Step [1/55], Loss: 0.2617\n",
      "Epoch [9/15], Step [1/55], Loss: 0.2329\n",
      "Epoch [10/15], Step [1/55], Loss: 0.2998\n",
      "Epoch [11/15], Step [1/55], Loss: 0.2317\n",
      "Epoch [12/15], Step [1/55], Loss: 0.2496\n",
      "Epoch [13/15], Step [1/55], Loss: 0.2380\n",
      "Epoch [14/15], Step [1/55], Loss: 0.2273\n",
      "Epoch [15/15], Step [1/55], Loss: 0.2214\n",
      "Train Accuracy: 53.86%\n",
      "--------------------------------\n",
      "Validation Accuracy: 41.82%\n",
      "--------------------------------\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch [1/15], Step [1/55], Loss: 0.2321\n",
      "Epoch [2/15], Step [1/55], Loss: 0.2597\n",
      "Epoch [3/15], Step [1/55], Loss: 0.2614\n",
      "Epoch [4/15], Step [1/55], Loss: 0.1769\n",
      "Epoch [5/15], Step [1/55], Loss: 0.2310\n",
      "Epoch [6/15], Step [1/55], Loss: 0.2513\n",
      "Epoch [7/15], Step [1/55], Loss: 0.2545\n",
      "Epoch [8/15], Step [1/55], Loss: 0.2366\n",
      "Epoch [9/15], Step [1/55], Loss: 0.2266\n",
      "Epoch [10/15], Step [1/55], Loss: 0.2524\n",
      "Epoch [11/15], Step [1/55], Loss: 0.2496\n",
      "Epoch [12/15], Step [1/55], Loss: 0.2859\n",
      "Epoch [13/15], Step [1/55], Loss: 0.2346\n",
      "Epoch [14/15], Step [1/55], Loss: 0.1857\n",
      "Epoch [15/15], Step [1/55], Loss: 0.2471\n",
      "Train Accuracy: 52.27%\n",
      "--------------------------------\n",
      "Validation Accuracy: 49.09%\n",
      "--------------------------------\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch [1/15], Step [1/55], Loss: 0.2244\n",
      "Epoch [2/15], Step [1/55], Loss: 0.2258\n",
      "Epoch [3/15], Step [1/55], Loss: 0.2410\n",
      "Epoch [4/15], Step [1/55], Loss: 0.2223\n",
      "Epoch [5/15], Step [1/55], Loss: 0.2396\n",
      "Epoch [6/15], Step [1/55], Loss: 0.2634\n",
      "Epoch [7/15], Step [1/55], Loss: 0.2497\n",
      "Epoch [8/15], Step [1/55], Loss: 0.2626\n",
      "Epoch [9/15], Step [1/55], Loss: 0.2135\n",
      "Epoch [10/15], Step [1/55], Loss: 0.2317\n",
      "Epoch [11/15], Step [1/55], Loss: 0.2212\n",
      "Epoch [12/15], Step [1/55], Loss: 0.2305\n",
      "Epoch [13/15], Step [1/55], Loss: 0.2273\n",
      "Epoch [14/15], Step [1/55], Loss: 0.2467\n",
      "Epoch [15/15], Step [1/55], Loss: 0.2233\n",
      "Train Accuracy: 56.59%\n",
      "--------------------------------\n",
      "Validation Accuracy: 38.18%\n",
      "--------------------------------\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch [1/15], Step [1/55], Loss: 0.2574\n",
      "Epoch [2/15], Step [1/55], Loss: 0.2533\n",
      "Epoch [3/15], Step [1/55], Loss: 0.2484\n",
      "Epoch [4/15], Step [1/55], Loss: 0.2095\n",
      "Epoch [5/15], Step [1/55], Loss: 0.2678\n",
      "Epoch [6/15], Step [1/55], Loss: 0.2550\n",
      "Epoch [7/15], Step [1/55], Loss: 0.2335\n",
      "Epoch [8/15], Step [1/55], Loss: 0.1905\n",
      "Epoch [9/15], Step [1/55], Loss: 0.2232\n",
      "Epoch [10/15], Step [1/55], Loss: 0.2629\n",
      "Epoch [11/15], Step [1/55], Loss: 0.2315\n",
      "Epoch [12/15], Step [1/55], Loss: 0.2352\n",
      "Epoch [13/15], Step [1/55], Loss: 0.2502\n",
      "Epoch [14/15], Step [1/55], Loss: 0.2446\n",
      "Epoch [15/15], Step [1/55], Loss: 0.2473\n",
      "Train Accuracy: 57.05%\n",
      "--------------------------------\n",
      "Validation Accuracy: 50.00%\n",
      "--------------------------------\n",
      "\n",
      "Overal Results\n",
      "Average Training Accurcy is : 54.45454545454546\n",
      "Average Validation Accurcy is : 46.727272727272734\n"
     ]
    }
   ],
   "source": [
    "X_tr_tensor = torch.tensor(np.float32(X_tr))\n",
    "y_tr_tensor = torch.tensor(np.float32(np.where(train_labels == 1, 1.0, 0.0)))\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(X_tr_tensor)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_ids)\n",
    "    train_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=train_subsampler)\n",
    "    valid_loader = DataLoader(TensorDataset(X_tr_tensor, y_tr_tensor), batch_size=batch_size, sampler=valid_subsampler)\n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "            for i, (inputs, targets) in enumerate(train_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs.ravel(), targets)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if (i+1) % num_total_steps-1 == 0:\n",
    "                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{num_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "    print(f'Train Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.where(outputs > 0.5, 1, 0).T.to(device)\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    valid_accuracies.append(100 * correct / total)\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "    print('--------------------------------')\n",
    "\n",
    "print('\\nOveral Results')\n",
    "print(f'Average Training Accurcy is : {np.mean(train_accuracies)}')\n",
    "print(f'Average Validation Accurcy is : {np.mean(valid_accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580861-fe09-4b34-9661-a47f64543775",
   "metadata": {},
   "source": [
    "#### Predict Test Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5019533-29c2-4185-89ec-17e25e559a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor = torch.tensor(np.float32(X_te)).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data_tensor)\n",
    "        \n",
    "predicted = torch.where(outputs > 0.5, 1, 0).T\n",
    "np.save('test_labels_RBF', predicted.cpu())\n",
    "predicted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
